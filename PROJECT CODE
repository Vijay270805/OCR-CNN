from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np

# Load the pre-trained model
model = load_model('custom_dataset_model.h5')

# Load the image
image_path = "/content/drive/MyDrive/MP DATASET/test/A/10.png"
image = load_img(image_path, target_size=(28, 28), color_mode='grayscale')  # Resize to match input size

# Preprocess the image
image_array = img_to_array(image)  # Convert to numpy array
image_array = image_array / 255.0  # Normalize pixel values to [0, 1]
image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension (1, 28, 28, 1)

# Predict the class
prediction = model.predict(image_array)  # Now passes a valid preprocessed image
predicted_class = np.argmax(prediction)
print(f"Predicted class: {predicted_class}")

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing import image_dataset_from_directory
import matplotlib.pyplot as plt

# Step 1: Load Your Dataset
train_dir = '/content/drive/MyDrive/mp dataset reduced/train'
test_dir = '/content/drive/MyDrive/mp dataset reduced/test'

# Load datasets
train_dataset = image_dataset_from_directory(
    train_dir,
    image_size=(28, 28),  # input shape of the model
    color_mode='grayscale',  # black&white
    batch_size=64
)

test_dataset = image_dataset_from_directory(
    test_dir,
    image_size=(28, 28),
    color_mode='grayscale',
    batch_size=64
)

# Extract class names before mapping
class_names = train_dataset.class_names  # Save class names before normalization
num_classes = len(class_names)  # Number of classes based on folder names

# Normalize datasets
def normalize(image, label):
    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]
    return image, label

train_dataset = train_dataset.map(normalize)
test_dataset = test_dataset.map(normalize)

#create the model
model = models.Sequential()

# First Layer(3x3 kernel)
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))

# Second Layer(3x3 kernel)
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

# Third Layer(3x3 kernel)
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

# Fourth Layer (1x1 kernel)
model.add(layers.Conv2D(256, (1, 1), activation='relu'))

# Flatten(array like structure)
model.add(layers.Flatten())

# connect layers(dense)
model.add(layers.Dense(128, activation='relu'))

# Output layer
model.add(layers.Dense(num_classes, activation='softmax'))

# Compiling
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Training
model.fit(train_dataset, epochs=50, validation_data =test_dataset)

# evaluation
test_loss, test_acc = model.evaluate(test_dataset)
print(f"Test accuracy: {test_acc * 100:.2f}%")

# Save
model.save('custom_dataset_model.h5')

# Predictions
for images, labels in test_dataset.take(1):
    image = images[0]
    label = labels[0]
    prediction = model.predict(tf.expand_dims(image, axis=0))
    predicted_class = tf.argmax(prediction[0])

    # display predictedd image
    plt.imshow(image.numpy().squeeze(), cmap='gray')
    plt.title(f"Predicted: {class_names[predicted_class.numpy()]}")
    plt.axis('off')
    plt.show()


